{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Data with Parquet & Dask\n",
    "We can use `dask` (rather than `pandas`) dataframes when we are merging datasets and they won't fit in memory. Here, the 12 monthly CSVs for 2019 are quite big. We could do a pd.merge, but this will result in memory issues and be **extremely** slow. Although pandas does use disk space when it runs out of memory, it is not particularly great at it. There will be a lot of [memory swapping](https://en.wikipedia.org/wiki/Memory_paging)\n",
    "\n",
    "We'd like to use dask.dataframes to just concat the 12 files *on disk*. This turns out to be surprisingly quick, as we will see\n",
    "\n",
    "In addition, we use `parquet` instead of CSVs. Parquet is a columnar data store which makes groupby/aggregations on a column much faster as you don't need to read all columns in a row when doing an aggregation like you would with a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV → Parquet\n",
    "1. Let's convert our monthly CSV files to parquet\n",
    "2. Then merge the 12 months into a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PARQUET_EXTENSION = \".parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_parquet(directory_from: str, directory_to: str):\n",
    "    \"\"\"convert csv in `directory_from` to parquet format in `directory_to`\"\"\"\n",
    "    csv_files = sorted([directory_from + f for f in os.listdir(directory_from)])\n",
    "    i = 0\n",
    "    for csv_file in csv_files:\n",
    "        i += 1\n",
    "        filename = csv_file.split(\"/\")[-1][:-4]\n",
    "        newfilepath = directory_to + filename + \".parquet\"\n",
    "        logging.debug(f\"parquetifying file {i} of {len(csv_files)}...\")\n",
    "\n",
    "        csv_df = pd.read_csv(csv_file)\n",
    "        csv_df.columns = [col.lower().replace(\" \", \"\") for col in csv_df.columns]\n",
    "        csv_df.dropna(inplace=True)  # remove NAs for trips to stations in the Bronx\n",
    "        csv_df[\"birthyear\"] = csv_df[\"birthyear\"].replace(\n",
    "            r\"\\\\N\", \"0\", regex=True\n",
    "        )  # replace \\N with string '0'\n",
    "        csv_df.to_parquet(newfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "convert_to_parquet(\"data/2019_csv/\", \"data/2019_parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_monthly_trips(year, directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a merged parquet file from parquet files in a directory\n",
    "    :param year: the year (int) to merge monthly data for. if None, then merge all files in directory\n",
    "    :param directory: a directory containing parquet files with identical schema (column names) across files\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if year:\n",
    "        range_start = str(year) + \"-01\"\n",
    "        range_end = str(year) + \"-13\"\n",
    "        month_files = sorted(\n",
    "            [\n",
    "                directory + f\n",
    "                for f in os.listdir(directory)\n",
    "                if range_start <= f <= range_end\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        month_files = sorted(\n",
    "            [\n",
    "                directory + f\n",
    "                for f in os.listdir(directory)\n",
    "                if f.endswith(PARQUET_EXTENSION)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    parquet_ddfs: list[dd.DataFrame] = []\n",
    "    for month_file in month_files:\n",
    "        if os.path.exists(month_file):\n",
    "            ddf = dd.read_parquet(month_file)\n",
    "            # ddf.astype(TRIPDATA_COLUMN_DTYPES)\n",
    "            ddf[\"birthyear\"] = ddf[\"birthyear\"].astype(\n",
    "                \"str\"\n",
    "            )  # some issue with birthyear in particular\n",
    "            parquet_ddfs.append(ddf)\n",
    "\n",
    "    all_trips = dd.concat(parquet_ddfs)\n",
    "    filename = str(year) if year else \"alltrips\"\n",
    "    all_trips.to_parquet(\n",
    "        directory + filename + PARQUET_EXTENSION,\n",
    "        schema={\"birthyear\": pa.string()},\n",
    "        engine=\"pyarrow\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_monthly_trips(2019, \"data/2019_parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "1. Read in the parquet file for 2019 we created\n",
    "2. Analyze effect of weather on number of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now read the 2019 parquet file to do data analysis\n",
    "TRIPS_COLUMNS = [\n",
    "    \"tripduration\",\n",
    "    \"starttime\",\n",
    "    \"stoptime\",\n",
    "    \"startstationid\",\n",
    "    \"endstationid\",\n",
    "    \"bikeid\",\n",
    "    \"usertype\",\n",
    "    \"birthyear\",\n",
    "    \"gender\",\n",
    "]\n",
    "\n",
    "trips = pd.read_parquet(\n",
    "    \"data/2019_parquet/2019.parquet\",\n",
    "    columns=TRIPS_COLUMNS,\n",
    "    engine=\"pyarrow\",\n",
    ").reset_index()\n",
    "trips.drop(trips.columns[0], axis=1, inplace=True)  # drop the dask index\n",
    "trips[\"starttime\"] = trips[\"starttime\"].astype(\"datetime64\")\n",
    "trips[\"stoptime\"] = trips[\"stoptime\"].astype(\"datetime64\")\n",
    "\n",
    "trips.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Examine relationship of trips with weather\n",
    "trips_per_day = (\n",
    "    trips.groupby(trips[\"starttime\"].dt.dayofyear)[\"tripduration\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"starttime\": \"dayofyear\", \"tripduration\": \"counttrips\"})\n",
    ")\n",
    "\n",
    "# read weather data\n",
    "weather = pd.read_csv(\"data/\" + \"GHCN-Daily-Cleaned.csv\", index_col=0)\n",
    "weather[\"DATE\"] = pd.to_datetime(weather[\"DATE\"])\n",
    "weather.set_index(weather[\"DATE\"])\n",
    "\n",
    "# get 2019 weather\n",
    "start_2019 = pd.to_datetime(\"2019-01-01\")\n",
    "end_2019 = pd.to_datetime(\"2019-12-31\")\n",
    "weather_2019 = weather.loc[\n",
    "    (weather[\"DATE\"] >= start_2019) & (weather[\"DATE\"] <= end_2019)\n",
    "]\n",
    "\n",
    "# merged weather + trips dataset\n",
    "w_trips = pd.merge(\n",
    "    weather_2019,\n",
    "    trips_per_day,\n",
    "    left_on=weather_2019[\"DATE\"].dt.dayofyear,\n",
    "    right_on=\"dayofyear\",\n",
    ")\n",
    "w_trips[\"Snowed on Day\"] = w_trips[\"SNWD\"].apply(lambda depth: depth > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot linear regression of weather conditions versus trips\n",
    "g = sns.lmplot(\n",
    "    data=w_trips,\n",
    "    x=\"TAVG_F\",\n",
    "    y=\"counttrips\",\n",
    "    hue=\"Snowed on Day\",\n",
    "    markers=[\"o\", \"*\"],\n",
    "    height=8,\n",
    "    aspect=1.5,\n",
    "    facet_kws={\"legend_out\": False},\n",
    ")\n",
    "plt.title(\"Daily Citibike Trips Increase with Temperature (2019)\")\n",
    "plt.xlabel(\"Average Temperature (°F)\")\n",
    "plt.ylabel(\"Number of Trips\")\n",
    "plt.legend(\n",
    "    labels=[\n",
    "        \"No Snow\",\n",
    "        \"Best fit (snow)\",\n",
    "        \"95% CI\",\n",
    "        \"Snow\",\n",
    "        \"Best fit (no snow)\",\n",
    "        \"95%CI\",\n",
    "    ]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
